# message-spam-classifier

## ğŸ“Œ Project Overview
This project develops a **spam SMS detection model** to classify text messages as either **spam** or **ham (legitimate)**.  
The dataset used is the classic **SMSSpamCollection** dataset with 5,572 messages (4,825 ham and 747 spam).  

The problem is highly imbalanced (â‰ˆ13% spam vs 87% ham), making it a realistic challenge for text classification.

---

## ğŸ“Š Exploratory Data Analysis
- **Class distribution**:  
  - Ham â†’ 4,825 (86.6%)  
  - Spam â†’ 747 (13.4%)  
- **Message length**:  
  - Ham: ~71 characters (short, conversational)  
  - Spam: ~138 characters (longer, promotional)  
- **Visual insights**:  
  - Count plots confirm imbalance  
  - Histograms show spam messages tend to be longer  

---

## ğŸ”§ Methodology

### Preprocessing
- Text normalization (lowercasing, punctuation removal)  
- Stopword removal + lemmatization  
- Feature extraction with **TF-IDF**  

### Models
- **Naive Bayes** (GridSearchCV tuned, alpha=1.0)  
  - Strength â†’ *zero false positives*  
- **Logistic Regression** (C=100, class_weight=balanced)  
  - Strength â†’ *handles imbalance effectively*  

---

## ğŸ“ˆ Model Performance

| Metric            | Naive Bayes | Logistic Regression |
|-------------------|-------------|----------------------|
| **Accuracy**      | 97.40%      | 98.03%              |
| **Spam Recall**   | 83%         | 89%                 |
| **F1-Score**      | 90%         | 92%                 |
| **False Positives** | 0         | 5                   |
| **False Negatives** | 25        | 17                  |

**Key Takeaways:**  
- Logistic Regression achieves **higher accuracy & recall**, minimizing missed spam.  
- Naive Bayes is more conservative (no false positives) but misses more spam messages.  

---

## âš ï¸ Challenges & Solutions
- **Imbalanced classes** â†’ Stratified sampling + class weights  
- **Noisy text** â†’ Regex cleaning + lemmatization  
- **Model tuning** â†’ Grid Search CV for hyperparameter optimization  

---

## âœ… Conclusion
- **Logistic Regression** is recommended for deployment:  
  - Accuracy: **98.03%**  
  - Spam Recall: **89%**  
  - Only 5 false positives vs 17 false negatives  
- In practice, minimizing **missed spam** is more critical than a few false alarms.  
- Future work â†’ Apply **SMOTE** to further reduce false negatives.  

---

## ğŸ›  Tech Stack
- Python 3.x  
- scikit-learn  
- NumPy, Pandas  
- Matplotlib, Seaborn  

---

## ğŸ“„ License
This project is for **academic purposes only**.  

âœ¨ If you find it useful, feel free to star â­ or fork ğŸ´ the repo!
